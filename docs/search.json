[
  {
    "objectID": "index.html#mat555e-statistical-data-analysis-for-comp.-sciences",
    "href": "index.html#mat555e-statistical-data-analysis-for-comp.-sciences",
    "title": "MAT555E",
    "section": "MAT555E-Statistical Data Analysis for Comp. Sciences",
    "text": "MAT555E-Statistical Data Analysis for Comp. Sciences\nCourse Instructor: Gül İnan\nCourse Summary:\n\nMAT555E is a graduate level course which aims to provide an introduction to commonly used statistical methods for inference and prediction problems in data analysis. The course will harmonize statistical theory and data analysis through examples. This course is designed such that:\n\n\n\nThe methods covered will include supervised learning algorithms with a focus on regression and classification problems and unsupervised learning algorithms with a focus on clustering problems,\n\n\nExtensions of these methods to high-dimensional settings will also be discussed, and\n\n\nApplication of these methods to data analysis problems and their software implementation will be done via Python (v. 3.10.6).\n\n\nAt the end of the semester, the students are expected:\n\n\nTo be fluent in the fundamental principles behind several statistical methods,\n\n\nTo be able to apply statistical methods to real life problems and data sets, and\n\n\nTo be prepared for more advanced coursework or scientific research in machine learning and related fields.\n\n\n\nCourse GitHub Organization: https://github.com/MAT555E-Fall22.\n\n\n\n\n\n\nImportant\n\n\n\nGitHub Classroom link for in-class activities: https://classroom.github.com/a/Wm3Zp3fj\n\n\nCourse Prerequisites:\n\nSince the course also touches on the mathematical and statistical theory behind the methods and uses Python for implementation, this course requires the following background:\n\n\nKnowledge of linear algebra, probability, statistics, and optimization,\n\n\nFamiliarity with Python’s Numpy, Pandas, Matplotlib, Seaborn, statsmodels, and Scikit-Learn libraries,\n\n\nFamiliarity with at least one computational document such as Jupyter Notebook, Google Colab, Visual Studio Code, or RStudio Quarto, and\n\n\nFamiliarity with Git commands and GitHub interface.\n\n\n\nClass Schedule:\nCRN 14267: Tuesdays between 14:30-17:30 at OBL3 (Computer Lab).\n\n\n\n\n\n\nImportant\n\n\n\nA kind request: If you are a statistics educator, especially in Turkey, and my course website, course materials, and instructional activities motivate you to adapt a similar philosophy for your class, PLEASE let me know it via inan@itu.edu.tr. I will also appreciate if you would acknowledge me on your course syllabus and website etc."
  },
  {
    "objectID": "instructor.html",
    "href": "instructor.html",
    "title": "MAT555E",
    "section": "",
    "text": "Instructor: Gül İnan\nE-mail: inan@itu.edu.tr\nOffice: Room 424 @ Department of Mathematics, Faculty of Arts and Sciences.\nOffice hour: You can ask me your questions right after the class or send me an e-mail for your queries."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "MAT555E",
    "section": "",
    "text": "MAT555E is a graduate level course which aims to provide an introduction to commonly used statistical methods for inference and prediction problems in data analysis. The course will harmonize statistical theory and data analysis through examples. This course is designed such that:\n\n\n\nTo provide the fundamental mathematical, statistical, and computational concepts behind supervised and unsupervised statistical learning methods and algorithms for inference and prediction.\n\n\nTo provide extensions of these methods to high-dimensional settings.\n\n\nTo provide the applications of these methods in real life data sets.\n\n\nTo provide the implementation of these methods in Python.\n\n\n\n\n\n\nThis is a graduate-level elective course open to all graduate students at ITU.\n\n\n\n3 local credits.\n\n\n\n\nSince the course also touches on the mathematical and statistical theory behind the methods and uses Python for implementation, this course requires the following background:\n\n\nKnowledge of linear algebra, probability, statistics, and optimization,\n\n\nFamiliarity with Python’s Numpy, Pandas, Matplotlib, Seaborn, statsmodels, and Scikit-Learn libraries,\n\n\nFamiliarity with at least one computational document such as Jupyter Notebook, Google Colab, Visual Studio Code, or RStudio Quarto, and\n\n\nFamiliarity with Git commands and GitHub interface.\n\n\n\n\n\n\nCRN 14267: Tuesdays between 14:30-17:30 at OBL3 (Computer Lab).\n\n\n\nCourse related all announcements will be done through Ninova. Lecture materials (lecture slides, code scripts, assignments etc) will be uploaded on Ninova and posted on GitHub organization of the course. Students are expected to bring their own portable computer to the class.\n\n\n\n1 midterm exam, 1 group-based paper presentation with a written-report, 1 final exam, and in-class performance.\n\n\n\nWe will closely follow the weekly schedule given below. However, weekly class schedules are subject to change depending on the progress we make as a class.\nWeek 1. Introduction. Framing a learning problem. Explanatory data analysis.\nWeek 2. Simple linear regression.\nWeek 3. Multiple linear regression.\nWeek 4. Introduction to classification. Logistic regression. Multinomial logistic regression.\nWeek 5. Naive Bayes. K-nearest neighbors.\nWeek 6. Linear discriminant analysis. Quadratic discriminant analysis.\nWeek 7. Cross-validation. Unsupervised pre-processing. Grid search and hyper-parameter tuning.\nWeek 8. ITU Fall Break.\nWeek 9. Model assessment and selection. Regularization methods for regression and classification problems. Ridge regression and lasso. Extensions to non-convex penalties.\nWeek 10. Moving beyond linearity. Polynomial regression. Regression splines.\nWeek 11. Tree based methods. Bagging, Random forests, and Boosting.\nWeek 12. Support vector machines.\nWeek 13. Unsupervised learning. Principal component analysis. Factor analysis.\nWeek 14. Clustering methods.\nWeek 15. Final review and examples.\n\n\n\n\nA student who completed this course successfully is expected:\n\n\nTo be fluent in the fundamental concepts and principles behind supervised and unsupervised statistical learning methods,\n\n\nTo be able to identify which method(s) might be suitable for conducting data analysis on specific real life data sets,\n\n\nTo get familiar with Python Scikit-Learn library, and\n\n\nTo be prepared for more advanced coursework or scientific research in machine learning and related fields.\n\n\nimmediately following the course, and/or a few months after the course.\n\n\n\n\nAll lecture materials.\n\n\n\n\nStudents are encouraged to consult the following sources on their own:\n\n\nHastie, T., Tibshirani, R., Friedman, J.H., and Friedman, J.H. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. New York: Springer. [Hard copy available at ITU Mustafa Inan Library with CALL #Q325.5 .H37 2009] [Available online at https://hastie.su.domains/ElemStatLearn/]\n\n\nJames, G., Witten, D., Hastie, T., and Tibshirani, R. (2021). An Introduction to Statistical Learning: With Applications in R. New York: Springer. [Available online at https://www.statlearning.com/ ].\n\n\nFan, J., Li, R., Zhang, C.H., and Zou, H. (2020). Statistical Foundations of Data Science. Chapman and Hall/CRC.\n\n\nDeisenroth, M.P., Faisal, A.A., and Ong, C. S. (2020). Mathematics for Machine Learning. Cambridge University Press. [Available online at https://mml-book.github.io/].\n\n\nVanderPlas, J. (2016). Python Data Science Handbook: Essential Tools for Working with Data. O’Reilly Media, Inc. [Available online at https://jakevdp.github.io/PythonDataScienceHandbook/].\n\n\nMüller, A.C., and Guido, S. (2016). Introduction to Machine Learning with Python: A Guide for Data Scientists. O’Reilly Media, Inc. [Available online at https://github.com/amueller/introduction_to_ml_with_python].\n\n\n\n\n\n\n\n\n\nMurphy, K.P. (2022). Probabilistic Machine Learning: An Introduction. MIT Press. [Available online at https://probml.github.io/pml-book/book1.html].\n\n\nBishop, C.M., Nasrabadi, N. M. (2006). Pattern Recognition and Machine Learning. New York: Springer. [Hard copy available at ITU Mechanical Eng. Library with CALL #Q327 .B52 2006]\n\n\n\n\n\n\n\nAccess to library e-sources remotely is possible with a library account. Users without a library account should apply for the library registration at Library register. After setting the web configurations given at Proxy only once on your computer, you will able to have an access to ITU Library e-sources.\n\n\n\n\nFor the official ITU Fall 2022 academic calendar, please visit:\nhttps://www.sis.itu.edu.tr/TR/ogrenci/akademik-takvim/akademik-takvimler/takvim2023/lisansustu-akademik-takvimi.php\nHere are some selected important dates in Fall 2022 semester:\nSeptember 19, 2022: First day of classes.\nSeptember 19-23, 2022: Add-drop week.\nOctober 29, 2022: Republic Day of Turkey (Saturday).\nNovember 7-11, 2022: ITU Fall Break (No classes).\nDecember 30, 2022: Last day of classes.\nJanuary 1, 2023: New year (Sunday).\nJanuary 02-15, 2023: Final exam week.\nI also honor other national and religious holidays. Students, who needs flexibility on individual-based studies overlapping with these special days, can inform me."
  },
  {
    "objectID": "syllabus.html#grading-policy",
    "href": "syllabus.html#grading-policy",
    "title": "MAT555E",
    "section": "Grading Policy",
    "text": "Grading Policy\n\n\n\nAssessment Method\nContribution to Final Grade\n\n\n\n\nIn-class performance\n10%\n\n\nMidterm exam\n30%\n\n\nPaper presentation\n30%\n\n\nFinal exam\n30%"
  },
  {
    "objectID": "syllabus.html#midterm-date-and-coverage",
    "href": "syllabus.html#midterm-date-and-coverage",
    "title": "MAT555E",
    "section": "Midterm date and coverage",
    "text": "Midterm date and coverage\n\nThe midterm will be on November 3rd, 2022 between 17:30-20:30. The midterm topics will cover whatever we have covered up to that week. The main aim of the midterm is to assess whether you are able to frame a data analysis problem, implement it, and report the results. The midterm will be hands-on and open-book exam. For that reason, you have to bring your own portable computer to the exam place."
  },
  {
    "objectID": "syllabus.html#group-based-paper-presentation-with-report-submission-date-and-coverage",
    "href": "syllabus.html#group-based-paper-presentation-with-report-submission-date-and-coverage",
    "title": "MAT555E",
    "section": "Group-based paper presentation (with report submission) date and coverage",
    "text": "Group-based paper presentation (with report submission) date and coverage\n\nThe group-based paper presentations will be on December 8th, 2022 between 17:30-20:30.\nWhen the semester starts, I urge you to visit Proceedings of Machine Learning Research and pick a paper published within nearly past three years (2022, 2021, and 2020) at AISTATS, ICML, or NeurIPS. I would like to hear about papers on explainable AI (XAI) a lot, but, feel free to select a topic on your own interest.\nI can anticipate that the content of these papers may be very heavy compared to our class topics. However, the main aim of group-based paper presentation (along with a report submission) is to asses whether you are able to read and understand a research problem recently carried out, and suggest an improvement (e.g., mathematical or computational) as an extension of the paper. The group size can be at most 2 and the presentation duration is 30 minutes (25 min. talk + 5 min. Q.A.)."
  },
  {
    "objectID": "syllabus.html#final-exam-date-and-coverage",
    "href": "syllabus.html#final-exam-date-and-coverage",
    "title": "MAT555E",
    "section": "Final exam date and coverage",
    "text": "Final exam date and coverage\n\nThe final exam date will be announced by ITU SIS later in December. The final exam topics will cover whatever we have covered throughout the semester with more advanced problems compared to midterm. The final exam will be hands-on and open-book exam. For that reason, you have to bring your own portable computer to the exam place."
  },
  {
    "objectID": "syllabus.html#final-exam-attendance-policy",
    "href": "syllabus.html#final-exam-attendance-policy",
    "title": "MAT555E",
    "section": "Final Exam Attendance Policy",
    "text": "Final Exam Attendance Policy\nThere is no VF rule to attend or not to attend the final exam."
  },
  {
    "objectID": "syllabus.html#make-up-exam-policy",
    "href": "syllabus.html#make-up-exam-policy",
    "title": "MAT555E",
    "section": "Make-Up Exam Policy",
    "text": "Make-Up Exam Policy\n\nThe students who miss either midterm exam or final exam due to a health problem can take a make-up exam as long as they have a valid medical report taken on the exam day. The medical report should be handed in immediately (within two days of its expiration). There will be NO make-up for missed in-class activities."
  },
  {
    "objectID": "syllabus.html#class-attendance-policy",
    "href": "syllabus.html#class-attendance-policy",
    "title": "MAT555E",
    "section": "Class Attendance Policy",
    "text": "Class Attendance Policy\nThe students must attend at least 70% of classes and are deemed responsible to manage his/her absences."
  },
  {
    "objectID": "syllabus.html#participation-policy",
    "href": "syllabus.html#participation-policy",
    "title": "MAT555E",
    "section": "Participation Policy",
    "text": "Participation Policy\n\nThe students are expected to ask and answer questions, participate in in-class activities, and show their interest and engagement in the class."
  },
  {
    "objectID": "syllabus.html#e-mail-policy",
    "href": "syllabus.html#e-mail-policy",
    "title": "MAT555E",
    "section": "E-mail Policy",
    "text": "E-mail Policy\nPlease:\n\nUse a proper descriptive subject line (which may consist of the course number MAT555E followed by a short phrase summarizing the subject of your e-mail).\nStart off your e-mail with a proper greeting, introduce yourself (give your name), then state your problem as short as possible.\nFinally, use a proper closing and then finish your e-mail with your first name and so on.\n\nFeel free to send me e-mails. But be sure you that give me enough time to get back to you.\n\n\n\n\n\n\nImportant\n\n\n\n\nE-mail messages sent after business hours and at weekends will be responded at the closest business hour.\nLastly, e-mails asking for grade grubbing at the end of the semester are not welcomed."
  },
  {
    "objectID": "syllabus.html#academic-honesty-policy",
    "href": "syllabus.html#academic-honesty-policy",
    "title": "MAT555E",
    "section": "Academic Honesty Policy",
    "text": "Academic Honesty Policy\n\nAt every stage of the academic life, every ITU student is responsible for obeying the academic honesty policy of ITU stated below:\n\nhttps://odek.itu.edu.tr/en/code-of-honor/ethics-in-university-life."
  },
  {
    "objectID": "syllabus.html#equity-diversity-and-inclusion",
    "href": "syllabus.html#equity-diversity-and-inclusion",
    "title": "MAT555E",
    "section": "Equity, Diversity, and Inclusion",
    "text": "Equity, Diversity, and Inclusion\n\nIn this class, I am committed to cultural and individual differences and diversity as including, but not limited to, age, disability, ethnicity, gender, gender identity, language, national origin, race, religion, culture, and socioeconomic status and I acknowledge the value of differences."
  },
  {
    "objectID": "syllabus.html#student-with-special-needs",
    "href": "syllabus.html#student-with-special-needs",
    "title": "MAT555E",
    "section": "Student with Special Needs",
    "text": "Student with Special Needs\n\nI truly care about that every student in my class feels that she/he involved in this class equally. If you are a student with special needs, please, let me know that how we can adjust the course environment, materials, and course assessment methods in accordance with your needs. Furthermore, you are also invited to contact the office of students with special needs at:\nhttp://engelsiz.itu.edu.tr/."
  },
  {
    "objectID": "lectures/Week_04/index.html",
    "href": "lectures/Week_04/index.html",
    "title": "Week 04",
    "section": "",
    "text": "Two different cultures: Statistical inference vs Statistical learning\nSupervised learning as a prediction problem\nSupervised learning workflow in practice\nRegression as a supervised learning problem\nIntroduction to scikit-learn API\nImplementing a linear regression model through LinearRegression module\nEvaluating model performance through regression metrics of sklearn.metrics"
  },
  {
    "objectID": "lectures/Week_04/index.html#lecture-slides",
    "href": "lectures/Week_04/index.html#lecture-slides",
    "title": "Week 04",
    "section": "Lecture Slides",
    "text": "Lecture Slides\n\nLecture slides and lab materials will be on Ninova. If you are one of the students coming to this class for auditing, please send your ITU ID number to me so that I can add you on the Ninova class.\nUnfortunately, I have decided not to post my lecture slides and lab materials on the web for a while. I have had some concerns that my data science related course contents are being used or replicated without acknowledging me or letting me know it. I prepare my notes from scratch and use other scholars’ content too. But, I always try to acknowledge them (ask for their permission, giving citation etc)."
  },
  {
    "objectID": "lectures/Week_03/Week_03_MLR.html",
    "href": "lectures/Week_03/Week_03_MLR.html",
    "title": "Lab 01",
    "section": "",
    "text": "In a study the association between advertising and sales of a particular product is being investigated. The advertising data set consists of sales of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: TV, radio, and newspaper.\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\n\n#import required libraries\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\n\n#import the data set in the .csv file into your your current session\nadvertise_df = pd.read_csv(\"https://www.statlearning.com/s/Advertising.csv\", index_col = 0)\n#index_col = column(s) to use as the row labels\nadvertise_df.head()\n\n\n\n\n\n  \n    \n      \n      TV\n      radio\n      newspaper\n      sales\n    \n  \n  \n    \n      1\n      230.1\n      37.8\n      69.2\n      22.1\n    \n    \n      2\n      44.5\n      39.3\n      45.1\n      10.4\n    \n    \n      3\n      17.2\n      45.9\n      69.3\n      9.3\n    \n    \n      4\n      151.5\n      41.3\n      58.5\n      18.5\n    \n    \n      5\n      180.8\n      10.8\n      58.4\n      12.9\n    \n  \n\n\n\n\n\n# We will fit three individual SLR and one MLR\n# Arrange the Y and X matrices for each case, respectively.\n# Define the common response\nY = advertise_df.sales\n# Arrange the first design matrix by including the intercept and TV variable\nX_tv = sm.add_constant(advertise_df.TV) \n#X_design_one\n# Arrange the second design matrix by including the intercept and radio variable\nX_radio = sm.add_constant(advertise_df.radio) \n#X_design_two\n# Arrange the third design matrix by including the intercept and newspaper variable\nX_news = sm.add_constant(advertise_df.newspaper) \n#X_design_three\n# Arrange the matrix for MLR by including the intercept and TV, radio and newspaper variables\nX = advertise_df[[\"TV\", \"radio\", \"newspaper\"]]\nX_full = sm.add_constant(X)\n\nLast week, we have investigated the relationship between sales and TV advertising.\n\n#https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html\n#class statsmodels.regression.linear_model.OLS\n#https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.fit.html\n#OLS.fit() method\n#Describe model\nmodel_tv = sm.OLS(Y, X_tv) #OLS class takes the data\n#Fit model and return results object\nresults_tv = model_tv.fit() \n#https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.html#statsmodels.regression.linear_model.RegressionResults\n#fit method returns another class named RegressionResults\n#hence results_tv object has its own class that is regression results\n#of course, regression results class has its own fit method\n#Based on the results, get predictions\npredictions_tv = results_tv.predict(X_tv)\n#of course, regression results class has its summary method\n#Get the results summary\nprint_model_tv = results_tv.summary()\nprint(print_model_tv)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  sales   R-squared:                       0.612\nModel:                            OLS   Adj. R-squared:                  0.610\nMethod:                 Least Squares   F-statistic:                     312.1\nDate:                Tue, 04 Oct 2022   Prob (F-statistic):           1.47e-42\nTime:                        10:24:32   Log-Likelihood:                -519.05\nNo. Observations:                 200   AIC:                             1042.\nDf Residuals:                     198   BIC:                             1049.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          7.0326      0.458     15.360      0.000       6.130       7.935\nTV             0.0475      0.003     17.668      0.000       0.042       0.053\n==============================================================================\nOmnibus:                        0.531   Durbin-Watson:                   1.935\nProb(Omnibus):                  0.767   Jarque-Bera (JB):                0.669\nSkew:                          -0.089   Prob(JB):                        0.716\nKurtosis:                       2.779   Cond. No.                         338.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n#as example, evaluate the loglik at OLS estimates. Yes, we can get the value in the tablo above.\n#model_tv.loglike(params=[7.0326,0.0475])\n\n\n#another example, rsquared is a property for results class. get the rsquare of the model\n#results_tv.rsquared\n#results_tv.mse_resid\n#results_tv.aic\n\nWe can also investigate the relationship between sales and radio. The results in the following table shows that a $1,000 increase in spending on radio advertising is associated with an increase in sales of around 203 units.\n\n#Describe model\nmodel_radio = sm.OLS(Y, X_radio) \n#Fit model and return results object\nresults_radio = model_radio.fit() \n#Based on the results, get predictions\npredictions_radio = results_radio.predict(X_radio) \n#Get the results summary\nprint_model_radio = results_radio.summary()\nprint(print_model_radio)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  sales   R-squared:                       0.332\nModel:                            OLS   Adj. R-squared:                  0.329\nMethod:                 Least Squares   F-statistic:                     98.42\nDate:                Tue, 04 Oct 2022   Prob (F-statistic):           4.35e-19\nTime:                        10:24:32   Log-Likelihood:                -573.34\nNo. Observations:                 200   AIC:                             1151.\nDf Residuals:                     198   BIC:                             1157.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          9.3116      0.563     16.542      0.000       8.202      10.422\nradio          0.2025      0.020      9.921      0.000       0.162       0.243\n==============================================================================\nOmnibus:                       19.358   Durbin-Watson:                   1.946\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               21.910\nSkew:                          -0.764   Prob(JB):                     1.75e-05\nKurtosis:                       3.544   Cond. No.                         51.4\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWe can also further investigate the relationship between sales and newspaper. The results in the following table shows that a $1,000 increase in spending on newspaper advertising is associated with an increase in sales of around 55 units.\n\n#Describe model\nmodel_news = sm.OLS(Y, X_news) \n#Fit model and return results object\nresults_news = model_news.fit() \n#Based on the results, get predictions\npredictions_news = results_news.predict(X_news) \n#Get the results summary\nprint_model_news = results_news.summary()\nprint(print_model_news)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  sales   R-squared:                       0.052\nModel:                            OLS   Adj. R-squared:                  0.047\nMethod:                 Least Squares   F-statistic:                     10.89\nDate:                Tue, 04 Oct 2022   Prob (F-statistic):            0.00115\nTime:                        10:24:32   Log-Likelihood:                -608.34\nNo. Observations:                 200   AIC:                             1221.\nDf Residuals:                     198   BIC:                             1227.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         12.3514      0.621     19.876      0.000      11.126      13.577\nnewspaper      0.0547      0.017      3.300      0.001       0.022       0.087\n==============================================================================\nOmnibus:                        6.231   Durbin-Watson:                   1.983\nProb(Omnibus):                  0.044   Jarque-Bera (JB):                5.483\nSkew:                           0.330   Prob(JB):                       0.0645\nKurtosis:                       2.527   Cond. No.                         64.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nOne problem associated with fitting seperate simple linear regression model for each predictor is that it is not clear that how to make a single prediction of sales given the three advertising media budgets.\nOn the other hand, each of the three regression equations ignores the other two media forming estimates for the regression coefficients.\nA better approach would be to fit a multiple linear regression to sales with three predictors such as:\n\n\\[sales_i = \\beta_0 +  \\beta_1 *TV_i + \\beta_2 * radio_i + \\beta_3 *newspaper_i + \\epsilon_i \\quad\\] for \\(i=1,2,\\ldots,200\\), where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\).\n\n#Describe model\nmodel_full = sm.OLS(Y, X_full) \n#Fit model and return results object\nresults_full = model_full.fit() \n#Based on the results, get predictions\npredictions_full = results_full.predict(X_full) \n#Get the results summary\nprint_model_full = results_full.summary()\nprint(print_model_full)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  sales   R-squared:                       0.897\nModel:                            OLS   Adj. R-squared:                  0.896\nMethod:                 Least Squares   F-statistic:                     570.3\nDate:                Tue, 04 Oct 2022   Prob (F-statistic):           1.58e-96\nTime:                        10:24:32   Log-Likelihood:                -386.18\nNo. Observations:                 200   AIC:                             780.4\nDf Residuals:                     196   BIC:                             793.6\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.9389      0.312      9.422      0.000       2.324       3.554\nTV             0.0458      0.001     32.809      0.000       0.043       0.049\nradio          0.1885      0.009     21.893      0.000       0.172       0.206\nnewspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n==============================================================================\nOmnibus:                       60.414   Durbin-Watson:                   2.084\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\nSkew:                          -1.327   Prob(JB):                     1.44e-33\nKurtosis:                       6.332   Cond. No.                         454.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nThe results above shows us the multiple regression coeffcient estimates when TV, radio, and newspaper advertising budgets are used to predict product sales.\nAs an example, the interpreation of \\(\\hat{\\beta}_2=0.1885\\) is follows: for a given amount of TV and newspaper advertising, spending an additional $ $1,000 $ on radio advertising is associated with approximately 189 units of additional sales.\nThe coefficient estimates \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_3\\) can be interpreted in a similar way.\nThe eye-catching point is that the coefficient of newspaper in SLR is 0.0547 (P-value close to 0), whereas the coefficient of newspaper in MLR is -0.0010 (P-value = 0.860).\nThis is a very good example that the coefficient of a variable in a SLR when other predictors are omitted from the model can be quite different in a MLR when other predictors included into the model !!!\nOne more time: In SLR, the slope term for newspaper represents the average increase in product sales associated with a $ $1,000 $ increase in newspaper advertising, ignoring TV and radio. In MLR, the slope term for newspaper represents the average increase in product sales associated with increasing newspaper spending by $ $1,000$ while holding TV and radio fixed.\nThe correlation between newspaper and radio is around 0.35. The correlation matrix result may imply that: in markets where we spend more on radio our sales will tend to be higher, we also tend to spend more on newspaper advertising in those same markets. Hence, in SLR which only examines sales versus newspaper, we will observe that higher values of newspaper tend to associated with higher values of sales, even though newspaper advertising is not directly associated with sales as MLR shows. So newspaper advertising is a surrogate for radio advertising: newspaper gets “credit” for the association between radio on sales.\n\n\nadvertise_df.corr()\n\n\n\n\n\n  \n    \n      \n      TV\n      radio\n      newspaper\n      sales\n    \n  \n  \n    \n      TV\n      1.000000\n      0.054809\n      0.056648\n      0.782224\n    \n    \n      radio\n      0.054809\n      1.000000\n      0.354104\n      0.576223\n    \n    \n      newspaper\n      0.056648\n      0.354104\n      1.000000\n      0.228299\n    \n    \n      sales\n      0.782224\n      0.576223\n      0.228299\n      1.000000\n    \n  \n\n\n\n\n\nA side note: in the results of MLR, we can see that loglikelihood is -386.18. Once we know the loglikelihood we can also calculate AIC and BIC values (they are already available in the output above).\nIf we compare the AIC, BIC values of all four models above, we can see that MLR is better than the others (the lower AIC, BIC, the better the model is).\n\n\nloglik = -386.18\nAIC = -2*loglik + 2*4 # four is the number of regression parameters\nBIC = -2*loglik + 4* np.log(200)\n\nOn the other hand, let’s test whether we should keep \\(X_2\\) (radio) and \\(X_3\\) (newspaper) in the full model or not. So, our reduced model is the one with \\(X_1\\) (TV) only.\n\\[\\begin{eqnarray}\nH_{0}:\\beta_2=\\beta_3=0  \\\\ \\nonumber\n\\end{eqnarray}\\]\nLet’s carry out an F test over full model.\n\nresults_full.compare_f_test(results_tv)[0:2] #f_value, p value\n\n(272.0406768057634, 2.8294869157010127e-57)\n\n\nP-value is close to zero, indicating that no at least one of the variables \\(X_2\\) (radio) and \\(X_3\\) (newspaper) should be in the model.\nCheck it with out formula since there is no example on the web:\n\\[\\begin{equation}\nF=\\frac{(RSS_0-RSS)/q}{RSS/(n-p-1)}  \\sim F_{q,(n-p-1)}.\n\\nonumber\n\\end{equation}\\]\n\nRSSo=sum(results_tv.resid**2)\nRSS=sum(results_full.resid**2)\nnum = (RSSo-RSS)/2 #q=2\ndenom =RSS/(200-4) #p=3, n=200\nF = num /denom\nF\n\n272.04067680576344"
  },
  {
    "objectID": "lectures/Week_03/Week_03_MLR.html#reference",
    "href": "lectures/Week_03/Week_03_MLR.html#reference",
    "title": "Lab 01",
    "section": "Reference",
    "text": "Reference\nJames, G., Witten, D., Hastie, T., and Tibshirani, R. (2021). An Introduction to Statistical Learning: With Applications in R. New York: Springer.\n\nimport session_info\nsession_info.show()\n\n\nClick to view session information\n-----\nnumpy               1.21.2\npandas              1.3.3\nsession_info        1.0.0\nstatsmodels         0.13.2\n-----\n\n\nClick to view modules imported as dependencies\nanyio                       NA\nappnope                     0.1.2\nattr                        21.2.0\nbabel                       2.9.1\nbackcall                    0.2.0\nbeta_ufunc                  NA\nbinom_ufunc                 NA\nbrotli                      1.0.9\ncertifi                     2021.05.30\nchardet                     4.0.0\ncharset_normalizer          2.0.0\ncolorama                    0.4.4\ncython_runtime              NA\ndateutil                    2.8.2\ndebugpy                     1.4.1\ndecorator                   5.1.0\nentrypoints                 0.3\ngoogle                      NA\nidna                        3.1\nipykernel                   6.4.1\nipython_genutils            0.2.0\njedi                        0.18.0\njinja2                      3.0.1\njoblib                      1.1.0\njson5                       NA\njsonschema                  3.2.0\njupyter_server              1.11.0\njupyterlab_server           2.8.1\nlz4                         4.0.0\nmarkupsafe                  2.0.1\nmpl_toolkits                NA\nnbclassic                   NA\nnbformat                    5.1.3\nnbinom_ufunc                NA\npackaging                   21.3\nparso                       0.8.2\npatsy                       0.5.2\npexpect                     4.8.0\npickleshare                 0.7.5\npkg_resources               NA\nprometheus_client           NA\nprompt_toolkit              3.0.20\nptyprocess                  0.7.0\npvectorc                    NA\npydev_ipython               NA\npydevconsole                NA\npydevd                      2.4.1\npydevd_concurrency_analyser NA\npydevd_file_utils           NA\npydevd_plugins              NA\npydevd_tracing              NA\npygments                    2.10.0\npyrsistent                  NA\npytz                        2021.1\nrequests                    2.26.0\nscipy                       1.7.1\nsend2trash                  NA\nsix                         1.16.0\nsniffio                     1.2.0\nsocks                       1.7.1\nstoremagic                  NA\nterminado                   0.12.1\ntornado                     6.1\ntraitlets                   5.1.0\ntyping_extensions           NA\nuritemplate                 4.1.1\nurllib3                     1.26.7\nwcwidth                     0.2.5\nwebsocket                   0.57.0\nzmq                         22.3.0\n\n \n-----\nIPython             7.27.0\njupyter_client      7.0.3\njupyter_core        4.8.1\njupyterlab          3.1.12\nnotebook            6.4.4\n-----\nPython 3.8.12 | packaged by conda-forge | (default, Sep 16 2021, 01:59:00) [Clang 11.1.0 ]\nmacOS-10.15.7-x86_64-i386-64bit\n-----\nSession information updated at 2022-10-04 10:24"
  },
  {
    "objectID": "lectures/Week_03/index.html",
    "href": "lectures/Week_03/index.html",
    "title": "Week 03",
    "section": "",
    "text": "Introduction to multiple linear regression\nLeast-squares estimation\nStatistical tests\nModel selection criteria\nImplementing a multiple linear regression model with Python statsmodels library"
  },
  {
    "objectID": "lectures/Week_03/index.html#lecture-slides",
    "href": "lectures/Week_03/index.html#lecture-slides",
    "title": "Week 03",
    "section": "Lecture Slides",
    "text": "Lecture Slides"
  },
  {
    "objectID": "lectures/Week_03/index.html#lab-materials",
    "href": "lectures/Week_03/index.html#lab-materials",
    "title": "Week 03",
    "section": "Lab Materials",
    "text": "Lab Materials\n\nMultiple Linear Regression Analysis"
  },
  {
    "objectID": "lectures/Week_02/index.html",
    "href": "lectures/Week_02/index.html",
    "title": "Week 02",
    "section": "",
    "text": "Introduction simple linear regression\nLeast-squares estimation\nBuilding a simple linear regression model\nImplementing a simple linear regression model with Python statsmodels library"
  },
  {
    "objectID": "lectures/Week_02/index.html#lecture-slides",
    "href": "lectures/Week_02/index.html#lecture-slides",
    "title": "Week 02",
    "section": "Lecture Slides",
    "text": "Lecture Slides"
  },
  {
    "objectID": "lectures/Week_02/index.html#lab-materials",
    "href": "lectures/Week_02/index.html#lab-materials",
    "title": "Week 02",
    "section": "Lab Materials",
    "text": "Lab Materials\n\nThe Sampling Distribution of OLS Estimators\nSimple Linear Regression Analysis"
  },
  {
    "objectID": "lectures/Week_02/Week_02_SLR.html",
    "href": "lectures/Week_02/Week_02_SLR.html",
    "title": "Lab 02",
    "section": "",
    "text": "In a study the association between advertising and sales of a particular product is being investigated. The advertising data set consists of sales of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: TV, radio, and newspaper.\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\n\n#import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\n\n#import the data set in the .csv file into your your current session\nadvertise_df = pd.read_csv(\"https://www.statlearning.com/s/Advertising.csv\", index_col = 0)\n#index_col = column(s) to use as the row labels\nadvertise_df.head()\n\n\n\n\n\n  \n    \n      \n      TV\n      radio\n      newspaper\n      sales\n    \n  \n  \n    \n      1\n      230.1\n      37.8\n      69.2\n      22.1\n    \n    \n      2\n      44.5\n      39.3\n      45.1\n      10.4\n    \n    \n      3\n      17.2\n      45.9\n      69.3\n      9.3\n    \n    \n      4\n      151.5\n      41.3\n      58.5\n      18.5\n    \n    \n      5\n      180.8\n      10.8\n      58.4\n      12.9\n    \n  \n\n\n\n\n\n#get a scatter plot of advertising budget in TV, radio, and newspaper vs sales, respectively.\nfig, axes = plt.subplots(1, 3, sharex=False, figsize=(10, 8))\nsns.regplot(ax=axes[0], x = advertise_df.TV, y = advertise_df.sales, ci = None)\nsns.regplot(ax=axes[1], x = advertise_df.radio, y = advertise_df.sales, ci = None, color = 'green')\nsns.regplot(ax=axes[2], x = advertise_df.newspaper, y = advertise_df.sales, ci = None, color = 'red');\n\n\n\n\nThe plot displays sales, in thousands of units, as a function of TV, radio, and newspaper budgets, in thousands of dollars, for 200 different markets. There exists a pretyy strong linear relationship between advertising budget in TV and sales. The lineartiy in other variables can be achieved via transformation etc.\nSince there exists approximately a linear relationthip between the TV and sales, let’s build a simple linear through regressing sales (Y) on TV advertising (X) as follows:\n\\[sales_i = \\beta_0 +  \\beta_1 *TV_i + \\epsilon_i \\quad\\] for \\(i=1,2,\\ldots,200\\), where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\).\n\n# get predictor and response\nX = advertise_df.TV\nY = advertise_df.sales\n\n# Add the intercept to the design matrix\nX = sm.add_constant(X) \n\n#https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.fit.html#statsmodels.regression.linear_model.OLS.fit\n#Describe model\nmodel = sm.OLS(Y, X) \n#Fit model and return results object\nresults = model.fit() \n#https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.html#statsmodels.regression.linear_model.RegressionResults\n#Based on the results, get predictions\npredictions = results.predict(X) \n#Get the results summary\nprint_model = results.summary()\nprint(print_model)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  sales   R-squared:                       0.612\nModel:                            OLS   Adj. R-squared:                  0.610\nMethod:                 Least Squares   F-statistic:                     312.1\nDate:                Sat, 01 Oct 2022   Prob (F-statistic):           1.47e-42\nTime:                        20:57:29   Log-Likelihood:                -519.05\nNo. Observations:                 200   AIC:                             1042.\nDf Residuals:                     198   BIC:                             1049.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          7.0326      0.458     15.360      0.000       6.130       7.935\nTV             0.0475      0.003     17.668      0.000       0.042       0.053\n==============================================================================\nOmnibus:                        0.531   Durbin-Watson:                   1.935\nProb(Omnibus):                  0.767   Jarque-Bera (JB):                0.669\nSkew:                          -0.089   Prob(JB):                        0.716\nKurtosis:                       2.779   Cond. No.                         338.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "lectures/Week_02/Week_02_SLR.html#reference",
    "href": "lectures/Week_02/Week_02_SLR.html#reference",
    "title": "Lab 02",
    "section": "Reference",
    "text": "Reference\nJames, G., Witten, D., Hastie, T., and Tibshirani, R. (2021). An Introduction to Statistical Learning: With Applications in R. New York: Springer.\n\nimport session_info\nsession_info.show()\n\n\nClick to view session information\n-----\nmatplotlib          3.4.3\nnumpy               1.21.2\npandas              1.3.3\nseaborn             0.11.2\nsession_info        1.0.0\nstatsmodels         0.13.2\n-----\n\n\nClick to view modules imported as dependencies\nPIL                         8.3.2\nanyio                       NA\nappnope                     0.1.2\nattr                        21.2.0\nbabel                       2.9.1\nbackcall                    0.2.0\nbeta_ufunc                  NA\nbinom_ufunc                 NA\nbrotli                      1.0.9\ncertifi                     2021.05.30\ncffi                        1.14.6\nchardet                     4.0.0\ncharset_normalizer          2.0.0\ncolorama                    0.4.4\ncycler                      0.10.0\ncython_runtime              NA\ndateutil                    2.8.2\ndebugpy                     1.4.1\ndecorator                   5.1.0\ndefusedxml                  0.7.1\nentrypoints                 0.3\ngoogle                      NA\nidna                        3.1\nipykernel                   6.4.1\nipython_genutils            0.2.0\nipywidgets                  7.6.5\njedi                        0.18.0\njinja2                      3.0.1\njoblib                      1.1.0\njson5                       NA\njsonschema                  3.2.0\njupyter_server              1.11.0\njupyterlab_server           2.8.1\nkiwisolver                  1.3.2\nlz4                         4.0.0\nmarkupsafe                  2.0.1\nmatplotlib_inline           NA\nmpl_toolkits                NA\nnbclassic                   NA\nnbformat                    5.1.3\nnbinom_ufunc                NA\npackaging                   21.3\nparso                       0.8.2\npatsy                       0.5.2\npexpect                     4.8.0\npickleshare                 0.7.5\npkg_resources               NA\nprometheus_client           NA\nprompt_toolkit              3.0.20\nptyprocess                  0.7.0\npvectorc                    NA\npydev_ipython               NA\npydevconsole                NA\npydevd                      2.4.1\npydevd_concurrency_analyser NA\npydevd_file_utils           NA\npydevd_plugins              NA\npydevd_tracing              NA\npygments                    2.10.0\npyparsing                   2.4.7\npyrsistent                  NA\npytz                        2021.1\nrequests                    2.26.0\nscipy                       1.7.1\nsend2trash                  NA\nsix                         1.16.0\nsniffio                     1.2.0\nsocks                       1.7.1\nstoremagic                  NA\nterminado                   0.12.1\ntornado                     6.1\ntraitlets                   5.1.0\ntyping_extensions           NA\nuritemplate                 4.1.1\nurllib3                     1.26.7\nwcwidth                     0.2.5\nwebsocket                   0.57.0\nzmq                         22.3.0\n\n \n-----\nIPython             7.27.0\njupyter_client      7.0.3\njupyter_core        4.8.1\njupyterlab          3.1.12\nnotebook            6.4.4\n-----\nPython 3.8.12 | packaged by conda-forge | (default, Sep 16 2021, 01:59:00) [Clang 11.1.0 ]\nmacOS-10.15.7-x86_64-i386-64bit\n-----\nSession information updated at 2022-10-01 20:57"
  },
  {
    "objectID": "lectures/Week_02/Week_02_Sampling_dist.html",
    "href": "lectures/Week_02/Week_02_Sampling_dist.html",
    "title": "Lab 01",
    "section": "",
    "text": "Generate \\(100\\) data points from the following model:\n\\[Y_i = 2 + (3 * X_i) + \\epsilon_i\n\\quad \\forall i = 1,\\ldots,100.\\]\n\nimport numpy as np\nfrom scipy import stats as stats\nfrom matplotlib import pyplot as plt\n\n\n#generate x data\nseed = 123 #did not assign\nn = 100\nx = stats.norm.rvs(loc = 0, scale = 4, size = n)\nbeta_0 = 2\nbeta_1 = 3\n#generate y_mean\ny_mean = beta_0 + beta_1*x\n#generate epsilon\nepsilon = stats.norm.rvs(loc = 0, scale = 6, size = n)\n#generate y with error terms\ny_data = y_mean + epsilon\n\nGet a scatter plot of the generated data, then plot the mean line (red) and the OLS regression line.\n\n#initialize layout\nfig, ax = plt.subplots(figsize = (8, 7))\n#add scatterplot\nax.scatter(x, y_data, s=50, alpha=1, color=\"steelblue\", edgecolors=\"k\")\n#fit linear regression via least squares with numpy.polyfit\n#it returns slope (b) and an intercept (a)\n# deg=1 means linear fit (i.e. polynomial of degree 1)\nb, a = np.polyfit(x, y_data, deg=1)\n#plot regression line\nax.plot(x, a + (b * x), color=\"blue\", lw=2.5)\n#fit a straight line passing through beta0 and beta1\nxseq = np.linspace(-10, 10, num=n)\nplt.plot(xseq, beta_0+(beta_1*xseq), color=\"red\", lw=2.5)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\");\n\n\n\n\nIf you repeatedly, say 5000 times, generate the data from the same data generating mechanism (equation above), store the intercept and slope parameter estiamtes for each run, plot the histogram of intercept estimates and histogram of slope estimates, respectively, you will see that OLS estimates have a bell-shaped curve centered around the true value set in the data generating mechanism.\n\nMC_run = 5000\n\n#create any empty list\n#generate MC_run times data sets\n#for each data sets, fit a SLR and get beta1hat\n\nbeta0_coef = []\nbeta1_coef = []\n\nfor i in range(MC_run):\n    n = 100\n    beta_0 = 2\n    beta_1 = 3\n    #generate x\n    x = stats.norm.rvs(loc = 0, scale = 4, size = n)\n    #generate y\n    y_data = beta_0 + beta_1*x + stats.norm.rvs(loc = 0, scale = 6, size = n)\n    #calculate beta1_ols for each data set\n    beta1, beta0 = np.polyfit(x, y_data, deg=1)\n    beta0_coef.append(beta0) \n    beta1_coef.append(beta1) \n\n\nimport seaborn as sns\nfig, axes = plt.subplots(1, 2, sharex=False, figsize=(8, 8))\nsns.histplot(ax=axes[0], x = beta0_coef, kde = True)\nsns.histplot(ax=axes[1], x = beta1_coef, kde = True)\naxes[0].set_xlabel(xlabel = r\"$\\hat{\\beta}_0$\")\naxes[1].set_xlabel(xlabel = r\"$\\hat{\\beta}_1$\")\naxes[0].set_title(r\"The sampling distribution of $\\hat{\\beta}_0$\")\naxes[1].set_title(r\"The sampling distribution of $\\hat{\\beta}_1$\")\naxes[0].axvline(x=2, color = 'red')\naxes[1].axvline(x=3, color = 'red');\n#plt.savefig('/Users/gulinan/Fall22_Courses/MAT555E/Week_02/images/sampling.png')\n\n\n\n\n\nimport session_info\nsession_info.show()\n\n\nClick to view session information\n-----\nsession_info        1.0.0\n-----\n\n\nClick to view modules imported as dependencies\nanyio                       NA\nappnope                     0.1.2\nattr                        21.2.0\nbabel                       2.9.1\nbackcall                    0.2.0\nbrotli                      1.0.9\ncertifi                     2021.05.30\nchardet                     4.0.0\ncharset_normalizer          2.0.0\ncolorama                    0.4.4\ncython_runtime              NA\ndateutil                    2.8.2\ndebugpy                     1.4.1\ndecorator                   5.1.0\nentrypoints                 0.3\ngoogle                      NA\nidna                        3.1\nipykernel                   6.4.1\nipython_genutils            0.2.0\njedi                        0.18.0\njinja2                      3.0.1\njson5                       NA\njsonschema                  3.2.0\njupyter_server              1.11.0\njupyterlab_server           2.8.1\nmarkupsafe                  2.0.1\nmpl_toolkits                NA\nnbclassic                   NA\nnbformat                    5.1.3\nnumpy                       1.21.2\npackaging                   21.3\nparso                       0.8.2\npexpect                     4.8.0\npickleshare                 0.7.5\npkg_resources               NA\nprometheus_client           NA\nprompt_toolkit              3.0.20\nptyprocess                  0.7.0\npvectorc                    NA\npydev_ipython               NA\npydevconsole                NA\npydevd                      2.4.1\npydevd_concurrency_analyser NA\npydevd_file_utils           NA\npydevd_plugins              NA\npydevd_tracing              NA\npygments                    2.10.0\npyrsistent                  NA\nrequests                    2.26.0\nsend2trash                  NA\nsix                         1.16.0\nsniffio                     1.2.0\nsocks                       1.7.1\nstoremagic                  NA\nterminado                   0.12.1\ntornado                     6.1\ntraitlets                   5.1.0\nuritemplate                 4.1.1\nurllib3                     1.26.7\nwcwidth                     0.2.5\nwebsocket                   0.57.0\nzmq                         22.3.0\n\n \n-----\nIPython             7.27.0\njupyter_client      7.0.3\njupyter_core        4.8.1\njupyterlab          3.1.12\nnotebook            6.4.4\n-----\nPython 3.8.12 | packaged by conda-forge | (default, Sep 16 2021, 01:59:00) [Clang 11.1.0 ]\nmacOS-10.15.7-x86_64-i386-64bit\n-----\nSession information updated at 2022-09-25 20:36"
  },
  {
    "objectID": "lectures/Week_01/index.html",
    "href": "lectures/Week_01/index.html",
    "title": "Week 01",
    "section": "",
    "text": "Brief introduction to statistical learning\nSimple techniques for data exploration"
  },
  {
    "objectID": "lectures/Week_01/index.html#lecture-slides",
    "href": "lectures/Week_01/index.html#lecture-slides",
    "title": "Week 01",
    "section": "Lecture Slides",
    "text": "Lecture Slides"
  },
  {
    "objectID": "lectures/Week_01/index.html#lab-materials",
    "href": "lectures/Week_01/index.html#lab-materials",
    "title": "Week 01",
    "section": "Lab Materials",
    "text": "Lab Materials\n\nExploratory Data Analysis-01\nExploratory Data Analysis-02"
  },
  {
    "objectID": "lectures/Week_01/Week_01_Eda_01.html",
    "href": "lectures/Week_01/Week_01_Eda_01.html",
    "title": "Lab 01",
    "section": "",
    "text": "The term data refers to the observations gathered on the characteristics of interest.\nFor example,\n\nA medical doctor may collect family history of a patient for a medical study,\nMarket research companies may collect demographic characteristics of people who voted for political candidates in a particular election, or\nA quality control engineer may collect operating data from an industrial manufacturing process.\n\nEach data point roots from a physical, demographical or behavioral phenomenon.\n\n\n\n\nThe data is generally organized and represented in a rectangular array of observed values, where\n\nEach row refers to the observations for a particular object (e.g., patients in a medical study), and\nEach column refers to the observations for a particular characteristic (attribute) (e.g., age of the patients, gender of the patients etc.) recorded for each row.\n\nMany statistical software/library requires a similar data structure (e.g., tabular data, tidy data) which is stored in a plain text file or in a spreadsheet file for data analysis.\n\n\n\n\n\nConsider the College.csv file which contains a number of variables for 777 different universites and colleges in the U.S.:\n\nPrivate : Public/private indicator\n\nApps : Number of applications received\n\nAccept : Number of applicants accepted\nEnroll : Number of new students enrolled\nTop10perc : New students from top 10% of high school class\nTop25perc : New students from top 25% of high school class\nF.Undergrad : Number of full-time undergraduates\nP.Undergrad : Number of part-time undergraduates\nOutstate : Out-of-state tuition\nRoom.Board : Room and board costs\nBooks : Estimated book costs\nPersonal : Estimated personal spending\nPhD : Percent of faculty with Ph.D.’s\nTerminal : Percent of faculty with terminal degree\nS.F.Ratio : Student/faculty ratio\nperc.alumni : Percent of alumni who donate\nExpend : Instructional expenditure per student\nGrad.Rate : Graduation rate\n\n\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\n\n#import the data set in the .csv file into your your current session\nimport pandas as pd\ncollege_df = pd.read_csv(\"https://www.statlearning.com/s/College.csv\", index_col = 0)\n#index_col = column(s) to use as the row labels\ncollege_df.head()\n\n\n\n\n\n  \n    \n      \n      Private\n      Apps\n      Accept\n      Enroll\n      Top10perc\n      Top25perc\n      F.Undergrad\n      P.Undergrad\n      Outstate\n      Room.Board\n      Books\n      Personal\n      PhD\n      Terminal\n      S.F.Ratio\n      perc.alumni\n      Expend\n      Grad.Rate\n    \n  \n  \n    \n      Abilene Christian University\n      Yes\n      1660\n      1232\n      721\n      23\n      52\n      2885\n      537\n      7440\n      3300\n      450\n      2200\n      70\n      78\n      18.1\n      12\n      7041\n      60\n    \n    \n      Adelphi University\n      Yes\n      2186\n      1924\n      512\n      16\n      29\n      2683\n      1227\n      12280\n      6450\n      750\n      1500\n      29\n      30\n      12.2\n      16\n      10527\n      56\n    \n    \n      Adrian College\n      Yes\n      1428\n      1097\n      336\n      22\n      50\n      1036\n      99\n      11250\n      3750\n      400\n      1165\n      53\n      66\n      12.9\n      30\n      8735\n      54\n    \n    \n      Agnes Scott College\n      Yes\n      417\n      349\n      137\n      60\n      89\n      510\n      63\n      12960\n      5450\n      450\n      875\n      92\n      97\n      7.7\n      37\n      19016\n      59\n    \n    \n      Alaska Pacific University\n      Yes\n      193\n      146\n      55\n      16\n      44\n      249\n      869\n      7560\n      4120\n      800\n      1500\n      76\n      72\n      11.9\n      2\n      10922\n      15\n    \n  \n\n\n\n\n\n#print the first 5 rows of the data\n#each row refers to a college\n#each column refers to an attribute for the colleges\ncollege_df.head()\n\n\n\n\n\n  \n    \n      \n      Private\n      Apps\n      Accept\n      Enroll\n      Top10perc\n      Top25perc\n      F.Undergrad\n      P.Undergrad\n      Outstate\n      Room.Board\n      Books\n      Personal\n      PhD\n      Terminal\n      S.F.Ratio\n      perc.alumni\n      Expend\n      Grad.Rate\n    \n  \n  \n    \n      Abilene Christian University\n      Yes\n      1660\n      1232\n      721\n      23\n      52\n      2885\n      537\n      7440\n      3300\n      450\n      2200\n      70\n      78\n      18.1\n      12\n      7041\n      60\n    \n    \n      Adelphi University\n      Yes\n      2186\n      1924\n      512\n      16\n      29\n      2683\n      1227\n      12280\n      6450\n      750\n      1500\n      29\n      30\n      12.2\n      16\n      10527\n      56\n    \n    \n      Adrian College\n      Yes\n      1428\n      1097\n      336\n      22\n      50\n      1036\n      99\n      11250\n      3750\n      400\n      1165\n      53\n      66\n      12.9\n      30\n      8735\n      54\n    \n    \n      Agnes Scott College\n      Yes\n      417\n      349\n      137\n      60\n      89\n      510\n      63\n      12960\n      5450\n      450\n      875\n      92\n      97\n      7.7\n      37\n      19016\n      59\n    \n    \n      Alaska Pacific University\n      Yes\n      193\n      146\n      55\n      16\n      44\n      249\n      869\n      7560\n      4120\n      800\n      1500\n      76\n      72\n      11.9\n      2\n      10922\n      15\n    \n  \n\n\n\n\n\n#Note that this is a pandas data frame\n#https://pandas.pydata.org/docs/reference/frame.html#\ntype(college_df)\n\npandas.core.frame.DataFrame\n\n\n\ncollege_df.columns\n\nIndex(['Private', 'Apps', 'Accept', 'Enroll', 'Top10perc', 'Top25perc',\n       'F.Undergrad', 'P.Undergrad', 'Outstate', 'Room.Board', 'Books',\n       'Personal', 'PhD', 'Terminal', 'S.F.Ratio', 'perc.alumni', 'Expend',\n       'Grad.Rate'],\n      dtype='object')\n\n\n\n#we can use some pandas data frame attributes to learn more about our data set\n#returns the column labels of the data frame\ncollege_df.columns\n\nIndex(['Private', 'Apps', 'Accept', 'Enroll', 'Top10perc', 'Top25perc',\n       'F.Undergrad', 'P.Undergrad', 'Outstate', 'Room.Board', 'Books',\n       'Personal', 'PhD', 'Terminal', 'S.F.Ratio', 'perc.alumni', 'Expend',\n       'Grad.Rate'],\n      dtype='object')\n\n\n\n#returns a tuple with number of rows and columns\ncollege_df.shape\n\n(777, 18)\n\n\n\n#returns an interger which is the product of rows and columns\ncollege_df.size\n\n13986\n\n\nNote that with the increasing variety of data that can be recorded electronically, not all data files have this traditional two-dimesional array format. For example, in medical records, some observations for some subjects may be images (e.g., dental x-rays, brain scans), hand-written prescriptions or a continuous streaming data over time such as monitoring the patient’s heart-rate, respiratory rate etc.\n\n\n\n\nA variable is a characteristic that can vary in value in objects of a data set.\n\n\n\n\n\nA variable is called quantitative when the measurement scale has numerical values that represent different magnitudes of the variable.\nExamples of quantitative variables are number of good friends, annual income, college GPA, age, and weight.\nA quantitative variable is discrete if it can take a set of distinct, separate values, such as the non-negative integers (0,1,2,3, …).\nExamples of discrete variables are one’s number of good friends, number of computers in household, and number of days playing a sport in the past week.\nA quantitative variable is continuous if it can take an infinite continuum of possible real number values.\nExamples of continuous variables are height, weight, age, distance a person walks in a day, winning time in a marathon race, and how long a cell phone works before it needs recharging.\n\n\n\n\n\nA variable is called categorical when the measurement scale is a set of categories.\nExamples of categorical variables are marital status (with categories such as single, married, divorced, widowed), primary mode of transportation to work (automobile, bicycle, bus, sub- way, walk), preferred destination for clothes shopping (downtown, Internet, mall, other), and favorite type of music (classical, country, folk, jazz, rap/hip-hop, rock).\nCategorical variables are often called qualitative.\nCategorical variables having only two categories, such as whether employed (yes, no), are called binary categorical variables.\nCategorical variables have two types of measurement scales.\nFor some categorical variables, the categories are unordered. The categories are then said to form a nominal scale.\nBy contrast, some categorical scales have a natural ordering of values. The categories form an ordinal scale.\nExamples are perceived happiness (not too happy, pretty happy, very happy), headache pain (none, slight, moderate, severe), and political philosophy (very liberal, slightly liberal, moderate, slightly conservative, very conservative).\n\n\n#returns a series with the data type of each column\ncollege_df.dtypes\n\nPrivate         object\nApps             int64\nAccept           int64\nEnroll           int64\nTop10perc        int64\nTop25perc        int64\nF.Undergrad      int64\nP.Undergrad      int64\nOutstate         int64\nRoom.Board       int64\nBooks            int64\nPersonal         int64\nPhD              int64\nTerminal         int64\nS.F.Ratio      float64\nperc.alumni      int64\nExpend           int64\nGrad.Rate        int64\ndtype: object\n\n\n\n#private variable is a binary variable\ncollege_df['Private'].unique()\n\narray(['Yes', 'No'], dtype=object)\n\n\n\n#get the frequency of each category of private variable\ncollege_df['Private'].value_counts()\n\nYes    565\nNo     212\nName: Private, dtype: int64\n\n\n\n#books is a discrete variable\ncollege_df['Books'].unique()\n\narray([ 450,  750,  400,  800,  500,  300,  660,  600,  650,  550,  900,\n         96,  350,  700,  540, 1000,  355,  630,  475, 1495, 2000,  410,\n        560,  860,  720,  425,  612,  480,  525,  526,  570, 2340,  654,\n        666,  380,  470,  680,  595,  520,  625,  530,  120,  490,  330,\n        580,  670,  795,  531,  465,  537,  640,  634,  920,  508,  528,\n        225,  618,  545,  575,  725,  375,  504, 1100,  200,  250,  850,\n        620,  759,  558,  515,  420,  616,  512,  436,  598,  554,  675,\n        610,  370, 1230,  955,  690,  569, 1200,  221,  576,  430,  385,\n        275,  711,  708, 1125,  635,  468,  518,  636,  790,  556,  687,\n        494,  476,  753,  714,  765,  495,  452, 1300,  646,  858,  541,\n        376,  535,  740,  585,  768, 1400,  110,  498,  639,  678,  605,\n        617])\n\n\n\n#get a general summary of the variables of the data set\ncollege_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nIndex: 777 entries, Abilene Christian University to York College of Pennsylvania\nData columns (total 18 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Private      777 non-null    object \n 1   Apps         777 non-null    int64  \n 2   Accept       777 non-null    int64  \n 3   Enroll       777 non-null    int64  \n 4   Top10perc    777 non-null    int64  \n 5   Top25perc    777 non-null    int64  \n 6   F.Undergrad  777 non-null    int64  \n 7   P.Undergrad  777 non-null    int64  \n 8   Outstate     777 non-null    int64  \n 9   Room.Board   777 non-null    int64  \n 10  Books        777 non-null    int64  \n 11  Personal     777 non-null    int64  \n 12  PhD          777 non-null    int64  \n 13  Terminal     777 non-null    int64  \n 14  S.F.Ratio    777 non-null    float64\n 15  perc.alumni  777 non-null    int64  \n 16  Expend       777 non-null    int64  \n 17  Grad.Rate    777 non-null    int64  \ndtypes: float64(1), int64(16), object(1)\nmemory usage: 115.3+ KB"
  },
  {
    "objectID": "lectures/Week_01/Week_01_Eda_01.html#how-to-explore-a-new-data-set",
    "href": "lectures/Week_01/Week_01_Eda_01.html#how-to-explore-a-new-data-set",
    "title": "Lab 01",
    "section": "How to explore a new data set?",
    "text": "How to explore a new data set?\n\nAssess the general characteristics of the data set, e.g.,\n\nHow many subjects/objects do we have? How many variables?\nWhat are the variable names? Are they meaningful? Are the names compatible with the software syntax we are going to use?\nWhat type is each variable - e.g., numeric (discrete or continuous), categorical?\nHow many unique values does each variable, especially categorical ones, have? (Very important to know before data splitting).\nAre there any missing observations?\n\n\n\n#we already checked most of them, but let's check\n#whether any missing values\n#college_df.isnull(): alias of isna()  \ncollege_df.isna().sum() \n\nPrivate        0\nApps           0\nAccept         0\nEnroll         0\nTop10perc      0\nTop25perc      0\nF.Undergrad    0\nP.Undergrad    0\nOutstate       0\nRoom.Board     0\nBooks          0\nPersonal       0\nPhD            0\nTerminal       0\nS.F.Ratio      0\nperc.alumni    0\nExpend         0\nGrad.Rate      0\ndtype: int64\n\n\n\nExamine descriptive statistics for each variable.\n\n\nDescriptive statistics are numbers computed from the data that present a summary of the variables.\n\n\nMeasures of Location\n\nThe most common measures of location are the Mean, the Median, the Mode, and the Quartiles.\nMean: the arithmetic average of all the observations. The mean equals the sum of all observations divided by the sample size.\nMedian: the middle-most value of the ranked set of observations so that half the observations are greater than the median and the other half is less. Median is a robust measure of central tendency.\nNote that the mean is very sensitive to outliers (extreme or unusual observations) whereas the median is not.\nMode: the most frequently occurring value in the data set. This makes more sense when attributes are not continuous.\nQuartiles: division points which split data into four equal parts after rank-ordering them.\n\nDivision points are called Q1 (the first quartile), Q2 (the second quartile or median), and Q3 (the third quartile)\nSimilarly, Deciles and Percentiles are defined as division points that divide the rank-ordered data into 10 and 100 equal segments.\n\n\n\n#get the mean values of numeric columns only\n#by default skipna=True\ncollege_df.mean(numeric_only = True)\n\nApps            3001.638353\nAccept          2018.804376\nEnroll           779.972973\nTop10perc         27.558559\nTop25perc         55.796654\nF.Undergrad     3699.907336\nP.Undergrad      855.298584\nOutstate       10440.669241\nRoom.Board      4357.526384\nBooks            549.380952\nPersonal        1340.642214\nPhD               72.660232\nTerminal          79.702703\nS.F.Ratio         14.089704\nperc.alumni       22.743887\nExpend          9660.171171\nGrad.Rate         65.463320\ndtype: float64\n\n\n\n#get the median values of numeric columns only\ncollege_df.median(numeric_only = True)\n\nApps           1558.0\nAccept         1110.0\nEnroll          434.0\nTop10perc        23.0\nTop25perc        54.0\nF.Undergrad    1707.0\nP.Undergrad     353.0\nOutstate       9990.0\nRoom.Board     4200.0\nBooks           500.0\nPersonal       1200.0\nPhD              75.0\nTerminal         82.0\nS.F.Ratio        13.6\nperc.alumni      21.0\nExpend         8377.0\nGrad.Rate        65.0\ndtype: float64\n\n\n\n#mode can be unique\ncollege_df['Accept'].mode()\n\n0    452\ndtype: int64\n\n\n\n#there can be multiple modes\ncollege_df['Apps'].mode()\n\n0     440\n1     663\n2    1006\ndtype: int64\n\n\n\n\nMeasures of Spread\n\nMeasures of location are not enough to capture all aspects of the variables.\nMeasures of dispersion are necessary to understand the variability of the data.\nThe most common measure of dispersion is the Variance, the Standard Deviation, the Interquartile Range and Range.\nVariance: measures how far data values lie from the mean. It is defined as the average of the squared differences between the mean and the individual data values.\nStandard Deviation: is the square root of the variance. It is defined as the average distance between the mean and the individual data values.\nInterquartile range (IQR): is the difference between Q3 and Q1. IQR contains the middle 50% of data.\nRange is the difference between the maximum and minimum values in the sample.\n\n\n#get the variance values of numeric columns only\n#by default skipna=True\ncollege_df.var(numeric_only = True)\n\nApps           1.497846e+07\nAccept         6.007960e+06\nEnroll         8.633684e+05\nTop10perc      3.111825e+02\nTop25perc      3.922292e+02\nF.Undergrad    2.352658e+07\nP.Undergrad    2.317799e+06\nOutstate       1.618466e+07\nRoom.Board     1.202743e+06\nBooks          2.725978e+04\nPersonal       4.584258e+05\nPhD            2.666086e+02\nTerminal       2.167478e+02\nS.F.Ratio      1.566853e+01\nperc.alumni    1.535567e+02\nExpend         2.726687e+07\nGrad.Rate      2.950737e+02\ndtype: float64\n\n\n\n#get the std values of numeric columns only\n#by default skipna=True\ncollege_df.std(numeric_only = True)\n\nApps           3870.201484\nAccept         2451.113971\nEnroll          929.176190\nTop10perc        17.640364\nTop25perc        19.804778\nF.Undergrad    4850.420531\nP.Undergrad    1522.431887\nOutstate       4023.016484\nRoom.Board     1096.696416\nBooks           165.105360\nPersonal        677.071454\nPhD              16.328155\nTerminal         14.722359\nS.F.Ratio         3.958349\nperc.alumni      12.391801\nExpend         5221.768440\nGrad.Rate        17.177710\ndtype: float64\n\n\n\n#or get a full summary for each variable\n#you need to manually calculate IQR=Q3-Q1\ncollege_df.describe().round(2)\n\n\n\n\n\n  \n    \n      \n      Apps\n      Accept\n      Enroll\n      Top10perc\n      Top25perc\n      F.Undergrad\n      P.Undergrad\n      Outstate\n      Room.Board\n      Books\n      Personal\n      PhD\n      Terminal\n      S.F.Ratio\n      perc.alumni\n      Expend\n      Grad.Rate\n    \n  \n  \n    \n      count\n      777.00\n      777.00\n      777.00\n      777.00\n      777.0\n      777.00\n      777.00\n      777.00\n      777.00\n      777.00\n      777.00\n      777.00\n      777.00\n      777.00\n      777.00\n      777.00\n      777.00\n    \n    \n      mean\n      3001.64\n      2018.80\n      779.97\n      27.56\n      55.8\n      3699.91\n      855.30\n      10440.67\n      4357.53\n      549.38\n      1340.64\n      72.66\n      79.70\n      14.09\n      22.74\n      9660.17\n      65.46\n    \n    \n      std\n      3870.20\n      2451.11\n      929.18\n      17.64\n      19.8\n      4850.42\n      1522.43\n      4023.02\n      1096.70\n      165.11\n      677.07\n      16.33\n      14.72\n      3.96\n      12.39\n      5221.77\n      17.18\n    \n    \n      min\n      81.00\n      72.00\n      35.00\n      1.00\n      9.0\n      139.00\n      1.00\n      2340.00\n      1780.00\n      96.00\n      250.00\n      8.00\n      24.00\n      2.50\n      0.00\n      3186.00\n      10.00\n    \n    \n      25%\n      776.00\n      604.00\n      242.00\n      15.00\n      41.0\n      992.00\n      95.00\n      7320.00\n      3597.00\n      470.00\n      850.00\n      62.00\n      71.00\n      11.50\n      13.00\n      6751.00\n      53.00\n    \n    \n      50%\n      1558.00\n      1110.00\n      434.00\n      23.00\n      54.0\n      1707.00\n      353.00\n      9990.00\n      4200.00\n      500.00\n      1200.00\n      75.00\n      82.00\n      13.60\n      21.00\n      8377.00\n      65.00\n    \n    \n      75%\n      3624.00\n      2424.00\n      902.00\n      35.00\n      69.0\n      4005.00\n      967.00\n      12925.00\n      5050.00\n      600.00\n      1700.00\n      85.00\n      92.00\n      16.50\n      31.00\n      10830.00\n      78.00\n    \n    \n      max\n      48094.00\n      26330.00\n      6392.00\n      96.00\n      100.0\n      31643.00\n      21836.00\n      21700.00\n      8124.00\n      2340.00\n      6800.00\n      103.00\n      100.00\n      39.80\n      64.00\n      56233.00\n      118.00\n    \n  \n\n\n\n\n\n\nMeasures of Skewness\n\nIn addition to the measures of location and dispersion, the arrangement of data or the shape of the data distribution is also of considerable interest.\nThe most ‘well-behaved’ distribution is a symmetric distribution where the mean and the median are coincident.\nThe symmetry is lost if there exists a tail in either direction.\nSkewness measures whether or not a distribution has a single long tail.\nThe sample skewness is measured as:\n\\[ \\frac{\\sum_{i}^{n}(x_i-\\bar{x})^3}{\\big(\\sum_{i}^{n}(x_i-\\bar{x})^2\\big)^\\frac{3}{2}}. \\]\n\n\n#import required libraries\n#https://docs.scipy.org/doc/scipy/reference/stats.html#\nimport numpy as np\nfrom scipy import stats as stats\nfrom matplotlib import pyplot as plt\n#generate large number of random variables from standard normal dist.\n#https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html#scipy.stats.norm\n#returns a numpy array\nx = stats.norm.rvs(loc = 0, scale = 1, size = 100000, random_state = 1773)\n#get a histogram of the data\nplt.hist(x, alpha = 0.5, edgecolor = 'black')\nplt.title('A symmetric distribution example', weight = \"bold\")\nplt.ylabel('Frequency')\nplt.xlabel('Values')\nplt.text(2, 30000, 'Skewness = 0', weight = \"bold\")  \nplt.show()\n\n\n\n\n\n#https://numpy.org/doc/stable/reference/generated/numpy.mean.html?highlight=mean#numpy.mean\n#https://stackoverflow.com/questions/40112487/attributeerror-numpy-ndarray-object-has-no-attribute-median\n#https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html\n#check out the mean and median, they should be close to each other\nprint('Mean=%.4f, Median=%.4f'% (np.mean(x), np.median(x)))\n\nMean=-0.0015, Median=0.0015\n\n\n\n#or get the other measures, investigate that in\n#the symmetric distributions, skewness is close to 0.\n#https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.describe.html?highlight=describe#scipy.stats.describe\nstats.describe(x)\n\nDescribeResult(nobs=100000, minmax=(-4.717908113448087, 4.364469403009946), mean=-0.0014900664839476684, variance=0.9997450526672419, skewness=-0.008348981045154868, kurtosis=0.01248822911019376)\n\n\n\n#generate large number of random variables from exponential dist. with scale param = 1\n#returns a numpy array\ny = stats.expon.rvs(scale = 1, size = 100000, random_state = 1773)\n#get a histogram of the data\nplt.hist(y, alpha = 0.5, bins = 50, edgecolor = 'black')\nplt.title('A right-skewed distribution example', weight = \"bold\")\nplt.ylabel('Frequency')\nplt.xlabel('Values')\n#plt.text(10, 60000, 'Skewness > 0', weight = \"bold\")    \nplt.show()\n\n\n\n\n\n#check out the mean and median, mean should be greater than median\nprint('Mean=%.4f, Median=%.4f'% (np.mean(y), np.median(y)))\n\nMean=0.9987, Median=0.6917\n\n\n\n#or get the other measures, look at, in\n#the right-skewed distributions, skewness is positive.\nstats.describe(y)\n\nDescribeResult(nobs=100000, minmax=(3.073149724927421e-06, 13.351146996791892), mean=0.9987103275783477, variance=0.9925253514338582, skewness=2.0073799405685375, kurtosis=6.232692611906376)\n\n\n\n#generate large number of random variables from a left skeed dist. with skeweness -5\n#returns a numpy array\nz = stats.skewnorm.rvs(a = -5, size = 100000, random_state = 1773)\n#get a histogram of the data\nplt.hist(z, alpha = 0.5, edgecolor = 'black')\nplt.title('A left-skewed distribution example', weight = \"bold\")\nplt.ylabel('Frequency')\nplt.xlabel('Values')\nplt.text(-4, 25000, 'Skewness < 0', weight = \"bold\")  \nplt.show()\n\n\n\n\n\n#check out the mean and median, mean should be smaller than median\nprint('Mean=%.4f, Median=%.4f'% (np.mean(z), np.median(z)))\n\nMean=-0.7820, Median=-0.6729\n\n\n\n#or get the other measures, look at in\n#the right-skewed distributions, skewness is positive.\nstats.describe(z)\n\nDescribeResult(nobs=100000, minmax=(-4.757740420390817, 0.6210253575361764), mean=-0.7819669683426621, variance=0.3870349600207956, skewness=-0.8625796090777151, kurtosis=0.7531576433616411)"
  },
  {
    "objectID": "lectures/Week_01/Week_01_Eda_01.html#box-cox-transformation",
    "href": "lectures/Week_01/Week_01_Eda_01.html#box-cox-transformation",
    "title": "Lab 01",
    "section": "Box-Cox Transformation",
    "text": "Box-Cox Transformation\n\nA Box-Cox transformation is a commonly used statistical method for transforming a non-normally distributed data into a more normally distributed data.\nThe basic idea behind this method is to find some value for \\(\\lambda\\) such that the transformed data is as close to normally distributed as possible, using the following formula which combines power and log transformations:\n\n\\[\ny(\\lambda)=\\begin{cases}\n      y^{\\lambda}-1, & \\quad \\lambda \\neq 0 \\\\\n      log(y) & \\quad \\lambda = 0 \\\\\n   \\end{cases}.\n\\]\n\n#https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html?highlight=stats%20boxcox#scipy.stats.boxcox\n#https://stats.stackexchange.com/questions/337527/parameter-lambda-of-box-cox-transformation-and-likelihood\n#perform Box-Cox transformation on y (original data)\ntransformed_y, best_lambda = stats.boxcox(y) \n#this the lambda that maximizes the log-likelihood function.\nprint('λ = %.4f' % (best_lambda))\n\nλ = 0.2664\n\n\n\n\ntransformed_y\n\narray([ 0.76327533, -0.26092415, -1.27886934, ...,  1.22406844,\n       -1.53643293, -1.7308228 ])\n\n\n\n#get a histogram of the transformed data\n#we can see that the transformed data follows a normal distribution.\nplt.hist(transformed_y, alpha = 0.5)\nplt.title('Histogram of the transformed data', weight = \"bold\")\nplt.ylabel('Frequency')\nplt.xlabel('Values')\nplt.show()"
  },
  {
    "objectID": "lectures/Week_01/Week_01_Eda_01.html#shapiro-wilk-test",
    "href": "lectures/Week_01/Week_01_Eda_01.html#shapiro-wilk-test",
    "title": "Lab 01",
    "section": "Shapiro-Wilk Test",
    "text": "Shapiro-Wilk Test\n\nShapiro-Wilk test is a non-parametric statistical test used for testing normality.\nThe hypothesis of the Shapiro-Wilk test is:\n\\(H_0\\): The sample \\(x_1,\\ldots, x_n\\) comes from a normally distributed population.\n\\(H_1\\): The sample \\(x_1, \\ldots,x_n\\) does not come from a normally distributed population.\nSince the test statistic does not follow a distribution (quantiles of the statistic is calculated through resampling based methods), its cut-off values are not tabulated. Here is the original paper.\nOne way to perform Shapiro-Wilk test is to rely on statistical software/libraries performing this test.\nWhen you get the observed value of the statistic and corresponding p-value of the test:\n\nIf the p-value is less than the chosen significance level (e.g., for an alpha level of 0.05), then \\(H_0\\) is rejected and there is evidence that the data tested are not normally distributed.\nOn the other hand, if the p-value is greater than the chosen significance level, then \\(H_0\\) cannot be rejected, then we can conclude that the data is from a normally distributed population.\n\n\n\n#https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html?highlight=shapiro#scipy.stats.shapiro\n#let's apply Shapiro-Wilk test to the data named x (we already know that\n#it comes from normal distribution\n#the function returns the observed test statistic and its corresponding p-value.\nshapiro_stat, p_value = stats.shapiro(x)\nprint('P-value = %.4f' % (p_value))\n\nP-value = 0.9466\n\n\nSince the p-value = 0.9466 is greater than 0.05, we fail to reject the null hypothesis. We do not have sufficient evidence to say that the data does not come from a normal distribution.\n\n#let's apply Shapiro-Wilk test to the data named y which is generated from exponential dist.\n#the function returns the osberved test statistic and its corresponding p-value.\nshapiro_stat, p_value = stats.shapiro(y)\nprint('P-value = %.4f' % (p_value))\n\nP-value = 0.0000\n\n\nSince the p-value = 0.0000 is less than 0.05, we reject the null hypothesis. We have sufficient evidence to say that the data does not come from a normal distribution.\nA note from scipy.stats.shapiro says that “For N > 5000 the W test statistic is accurate but the p-value may not be.”"
  },
  {
    "objectID": "lectures/Week_01/Week_01_Eda_01.html#measures-of-correlation",
    "href": "lectures/Week_01/Week_01_Eda_01.html#measures-of-correlation",
    "title": "Lab 01",
    "section": "Measures of Correlation",
    "text": "Measures of Correlation\n\nCorrelation describes the degree and direction of the linear relationship between two variables, \\(X\\) and \\(Y\\).\nLet we have a random sample of size \\(n\\) observed on \\(X\\) and \\(Y\\) variables where the data pairs are denoted by \\((x_1,y_1),\\ldots,(x_n,y_n)\\), the sample correlation coefficient is defined as:\n\n\\[ r_{xy}=\\frac{\\sum_{i}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\big(\\sum_{i}^{n}(x_i-\\bar{x})^2\\sum_{i}^{n}(y_i-\\bar{y})^2\\big)}}. \\]\n\nThe correlation falls between -1 and 1 (\\(-1 \\leq r_{xy} \\leq 1\\)).\nIf \\(r_{xy} > 0\\), the association is positive,\nIf \\(r_{xy} < 0\\), the association is negative, and\nIf \\(r_{xy} = 0\\), it indicates no linear relationship.\nThe larger the absolute value \\(r_{xy}\\), the stronger the association.\n\nLet’s investigate how the scatter plot changes as the correlation changes.\n\n# Importing the necessary libraries\nfrom scipy import stats as stats\nfrom matplotlib import pyplot as plt\nimport ipywidgets as widgets\n\nplt.style.use('seaborn-whitegrid')\nplt.rcParams['figure.figsize']=14,6\n\n# define a corr function with flexible corr input\ndef corr_widget(corr = 0):\n    \n    # Defining the mean vector\n    mean_x_y = np.array([20,30])\n\n    # Setting sd and corr \n\n    sigma_x = 4\n    sigma_y = 5\n    corr_x_y = corr\n\n    # Defining the variance-covariance matrix\n\n    cov_x_y = np.array([[sigma_x**2, corr_x_y*sigma_x*sigma_y], [corr_x_y*sigma_x*sigma_y, sigma_y**2]])\n\n    # Generating a data based on bivariate normal distribution\n    # with given mean vector and variance-covariance matrix\n    \n    data = stats.multivariate_normal.rvs(mean = mean_x_y, cov = cov_x_y, size = 100) #seed throwed an error\n\n    # Plotting the generated samples\n    \n    plt.plot(data[:,0], data[:,1], 'o', c = 'blue')\n    plt.title(f'Correlation between X and Y = {corr_x_y}')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n\n    plt.show()\n\n#turn it into a widget\ncorr_wid = widgets.FloatSlider(min = -1, max = 1, step = 0.1, value = 0, description = \"$r_x_y$\")\n#display(corr_wid)    \n\nNow, play with the following slider to see how correlation changes.\n\nwidgets.interact(corr_widget, corr = corr_wid);\n\n\n\n\n\nimport session_info\nsession_info.show()\n\n\nClick to view session information\n-----\nipywidgets          7.6.5\nmatplotlib          3.4.3\nnumpy               1.21.2\npandas              1.3.3\nscipy               1.7.1\nsession_info        1.0.0\n-----\n\n\nClick to view modules imported as dependencies\nPIL                         8.3.2\nanyio                       NA\nappnope                     0.1.2\nattr                        21.2.0\nbabel                       2.9.1\nbackcall                    0.2.0\nbeta_ufunc                  NA\nbinom_ufunc                 NA\nbrotli                      1.0.9\ncertifi                     2021.05.30\ncffi                        1.14.6\nchardet                     4.0.0\ncharset_normalizer          2.0.0\ncolorama                    0.4.4\ncycler                      0.10.0\ncython_runtime              NA\ndateutil                    2.8.2\ndebugpy                     1.4.1\ndecorator                   5.1.0\ndefusedxml                  0.7.1\nentrypoints                 0.3\ngoogle                      NA\nidna                        3.1\nipykernel                   6.4.1\nipython_genutils            0.2.0\njedi                        0.18.0\njinja2                      3.0.1\njson5                       NA\njsonschema                  3.2.0\njupyter_server              1.11.0\njupyterlab_server           2.8.1\nkiwisolver                  1.3.2\nmarkupsafe                  2.0.1\nmatplotlib_inline           NA\nmpl_toolkits                NA\nnbclassic                   NA\nnbformat                    5.1.3\nnbinom_ufunc                NA\npackaging                   21.3\nparso                       0.8.2\npexpect                     4.8.0\npickleshare                 0.7.5\npkg_resources               NA\nprometheus_client           NA\nprompt_toolkit              3.0.20\nptyprocess                  0.7.0\npvectorc                    NA\npydev_ipython               NA\npydevconsole                NA\npydevd                      2.4.1\npydevd_concurrency_analyser NA\npydevd_file_utils           NA\npydevd_plugins              NA\npydevd_tracing              NA\npygments                    2.10.0\npyparsing                   2.4.7\npyrsistent                  NA\npytz                        2021.1\nrequests                    2.26.0\nsend2trash                  NA\nsix                         1.16.0\nsniffio                     1.2.0\nsocks                       1.7.1\nstoremagic                  NA\nterminado                   0.12.1\ntornado                     6.1\ntraitlets                   5.1.0\nurllib3                     1.26.7\nwcwidth                     0.2.5\nwebsocket                   0.57.0\nzmq                         22.3.0\n\n \n-----\nIPython             7.27.0\njupyter_client      7.0.3\njupyter_core        4.8.1\njupyterlab          3.1.12\nnotebook            6.4.4\n-----\nPython 3.8.12 | packaged by conda-forge | (default, Sep 16 2021, 01:59:00) [Clang 11.1.0 ]\nmacOS-10.15.7-x86_64-i386-64bit\n-----\nSession information updated at 2022-09-20 16:35"
  },
  {
    "objectID": "lectures/Week_01/Week_01_Eda_02.html",
    "href": "lectures/Week_01/Week_01_Eda_02.html",
    "title": "Lab 02",
    "section": "",
    "text": "Where possible, certainly for any variable of particular interest-examine exploratory visualizations."
  },
  {
    "objectID": "lectures/Week_01/Week_01_Eda_02.html#tools-for-displaying-single-variables",
    "href": "lectures/Week_01/Week_01_Eda_02.html#tools-for-displaying-single-variables",
    "title": "Lab 02",
    "section": "Tools for Displaying Single Variables",
    "text": "Tools for Displaying Single Variables\n\nHistograms\n\nHistograms are the most common graphical tool to represent continuous data.\nA histogram divides the data into bins, counts the number of data points falling into each bin, and shows the bins on the horizontal axis and the frequency on the vertical axis.\nThe bin width (sometimes called class width) has an impact on the shape of the histogram.\n\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\n\n#import the data set in the .csv file into your your current session\nimport pandas as pd\ncollege_df = pd.read_csv(\"https://www.statlearning.com/s/College.csv\", index_col = 0)\n\n\n#https://matplotlib.org/stable/tutorials/introductory/pyplot.html\nimport matplotlib.pyplot as plt\n#For college_df, produce some histograms for a few of the quantitative variables.\n#change numbers of bins \nn_bins = 50\n#the frequency of number applications decreases as magnitude increases\nplt.hist(college_df['Apps'], bins = n_bins, color = 'red', alpha = 0.5, edgecolor = 'black')\nplt.title('The histogram of number of applications received')\nplt.show()\n\n\n\n\n\n\nSubplotting with matplotlib\nUsing the College data set, let’s create a 2x2 subplot with the following variables:\n\nApps\nperc.alumni\nS.F.Ratio\nExpend\n\nwhere each subplot is a histogram.\n\n#create subplots with matplotlib\n#https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html?highlight=subplots\nimport matplotlib.pyplot as plt\n#Create a figure and a set of subplots\n#returns a figure and array of axes\nfigure, axs = plt.subplots(2, 2, figsize = (16,8)) #figsize: width, height, respectively.\n#axs object is a 2 by 2 numpy array\n#type(axs)\n#axs\n\n\n\n\n\nimport matplotlib.pyplot as plt\n#Create a figure and a set of subplots\n#returns a figure and array of axes\nfigure, axs = plt.subplots(2, 2, figsize = (16,8)) #figsize: width, height, respectively.\n#fill-in the subplots\nn_bins = 50\n##A question: Why not plt hist??\n#the frequency of number applications decreases as magnitude increases\naxs[0,0].hist(college_df['Apps'], bins = n_bins, color = 'red', alpha = 0.5, edgecolor = 'black')\naxs[0,0].set_title('The histogram of number of applications received')\naxs[0,1].hist(college_df['Accept'], bins = n_bins, color = 'green', alpha = 0.5, edgecolor = 'black')\naxs[0,1].set_title('The histogram of number of applications accepted')\n#the distribution of estimated book costs is symmetric. there a few expensive books used at colleges.\naxs[1,0].hist(college_df['Books'], bins = n_bins, color = 'orange', alpha = 0.5, edgecolor = 'black')\naxs[1,0].set_title('The histogram of estimated book costs')\naxs[1,1].hist(college_df['Expend'], bins = n_bins, alpha = 0.5, edgecolor = 'black')\naxs[1,1].set_title('The histogram of instructional expenditure per student')\nplt.show(figure) #plt.show() turns off printing the data array etc\n#you can change the number of bins, as you wish\n#could not give y axis title for each subplot\n\n\n\n\n\n\nDensity Plots\n\nA density plot is a smoothed, continuous version of a histogram estimated from the data.\nA point is drawn at the top of every individual rectangular bin and all of these points are then connected together to make a single smooth density estimation. (Try to make a connection between Rieman sums and its limit as the number of bins goes to infinity (or width of the bins approaches to zero) and histograms).\nThe height of the curve is adjusted so that the total area under the curve integrates to one.\n\n\n\nHistograms, Density Plots with seaborn\n\n#https://seaborn.pydata.org/generated/seaborn.displot.html\n#no submodule\nimport seaborn as sns\n#draw a histogram with seaborn, kind=\"hist\" is default\nsns.displot(x = college_df.Expend, kind = \"hist\") \n#draw a denstiy plot with seaborn with kernel density\nsns.displot(x = college_df.Expend, kind = \"kde\")\n#overlay histogram with a density plot \nsns.displot(x = college_df.Expend, kde = True);"
  },
  {
    "objectID": "lectures/Week_01/Week_01_Eda_02.html#tools-for-displaying-a-single-variable-with-repect-to-a-categorical-variable",
    "href": "lectures/Week_01/Week_01_Eda_02.html#tools-for-displaying-a-single-variable-with-repect-to-a-categorical-variable",
    "title": "Lab 02",
    "section": "Tools for Displaying a Single Variable with repect to a Categorical Variable",
    "text": "Tools for Displaying a Single Variable with repect to a Categorical Variable\n\n#Each type of plot can be drawn separately for subsets of data using **hue mapping**\n#Investigate how expenditure is changing with respect to school type\nsns.displot(data = college_df, x = \"Expend\", hue = \"Private\", kind = \"hist\") \nsns.displot(data = college_df, x = \"Expend\", hue = \"Private\", kind = \"kde\");\n#as you can see instructional expenditure is  higher in private schools compared to non-private ones\n\n\n\n\n\n\n\n\n#alternatively you can prefer stacked plots\nsns.displot(data = college_df, x = \"Expend\", hue = \"Private\", kind = \"hist\", multiple = \"stack\");\n\n\n\n\n\n#faceting, map the expenditure variable into private variable with col argument\nsns.displot(data = college_df, x = \"Expend\", col = \"Private\", kind = \"hist\") \nsns.displot(data = college_df, x = \"Expend\", col = \"Private\", kind = \"kde\");\n\n\n\n\n\n\n\n\nBoxplots\n\nBoxplots are used to describe the shape of data distribution.\nWe can also use the boxplots to describe the shape of data distribution with respect to the levels of a categorical variable.\nIt also enables us to identify outliers where an observation is considered as an outlier if it is either less than Q1 - 1.5 IQR or greater than Q3 + 1.5 IQR, with IQR = Q3 - Q1.\nThis rule is conservative and often too many points are identified as outliers. Hence sometimes only those points outside of [Q1 - 3 IQR, Q3 + 3 IQR] are only identified as outliers.\n\nUsing the College data set, we will create a new categorical variable, called Elite, by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%.\n\n#attach a new column to the college df\n\nimport pandas as pd\ncollege_df = pd.read_csv(\"https://www.statlearning.com/s/College.csv\", index_col = 0)\n\n#this should be much more smarter.\n#i tried boolean indexing below, but it worked first, but,\n#it did not work later.\ncollege_df['Elite'] = \"No\"\ncollege_df.Elite[college_df.Top10perc>50] = \"Yes\"\n\n#alternatively, a bit longer way to add a new column to a data frame\n#elite = []\n#for i in college_df['Top10perc']:\n#        if i > 50 :    elite.append('Yes')\n#        else:          elite.append('No')\n        \n#college_df['Elite'] = elite \ncollege_df.head()\n\n\n\n\n\n  \n    \n      \n      Private\n      Apps\n      Accept\n      Enroll\n      Top10perc\n      Top25perc\n      F.Undergrad\n      P.Undergrad\n      Outstate\n      Room.Board\n      Books\n      Personal\n      PhD\n      Terminal\n      S.F.Ratio\n      perc.alumni\n      Expend\n      Grad.Rate\n      Elite\n    \n  \n  \n    \n      Abilene Christian University\n      Yes\n      1660\n      1232\n      721\n      23\n      52\n      2885\n      537\n      7440\n      3300\n      450\n      2200\n      70\n      78\n      18.1\n      12\n      7041\n      60\n      No\n    \n    \n      Adelphi University\n      Yes\n      2186\n      1924\n      512\n      16\n      29\n      2683\n      1227\n      12280\n      6450\n      750\n      1500\n      29\n      30\n      12.2\n      16\n      10527\n      56\n      No\n    \n    \n      Adrian College\n      Yes\n      1428\n      1097\n      336\n      22\n      50\n      1036\n      99\n      11250\n      3750\n      400\n      1165\n      53\n      66\n      12.9\n      30\n      8735\n      54\n      No\n    \n    \n      Agnes Scott College\n      Yes\n      417\n      349\n      137\n      60\n      89\n      510\n      63\n      12960\n      5450\n      450\n      875\n      92\n      97\n      7.7\n      37\n      19016\n      59\n      Yes\n    \n    \n      Alaska Pacific University\n      Yes\n      193\n      146\n      55\n      16\n      44\n      249\n      869\n      7560\n      4120\n      800\n      1500\n      76\n      72\n      11.9\n      2\n      10922\n      15\n      No\n    \n  \n\n\n\n\n\n#https://seaborn.pydata.org/generated/seaborn.boxplot.html\n#compare the graduation rates with respect to school's elite status\nsns.boxplot(data = college_df, x = \"Elite\", y = \"Grad.Rate\");\n#vertical box-plot\n#as you can see in the elite schools, the graduate rate is higher compared to non-elite ones\n#we can see a few outliers in the non-elite schools. while graduation rate is\n#very high in some non-elite schools, graduation rate is very low in some non-elite schools\n\n\n\n\n\n#this is an example that column names should not involve .\n#college_df.Grad.Rate.max()#will not work\n#hmmm\ncollege_df[\"Grad.Rate\"].max()\n\n118\n\n\n\n#produce side-by-side boxplots of Outstate versus Private.\n#horizontal box-plot\nsns.boxplot(data = college_df, x = \"Outstate\", y = \"Private\");\n#as you can see in private schools, out-state tution fees are also very high.\n#if you born in Arizona, but, want to study in a private school, such as in Boston,\n#you are more likely to pay for higher tution rates."
  },
  {
    "objectID": "lectures/Week_01/Week_01_Eda_02.html#tools-for-displaying-relationships-between-two-variables",
    "href": "lectures/Week_01/Week_01_Eda_02.html#tools-for-displaying-relationships-between-two-variables",
    "title": "Lab 02",
    "section": "Tools for Displaying Relationships Between Two Variables",
    "text": "Tools for Displaying Relationships Between Two Variables"
  },
  {
    "objectID": "lectures/Week_01/Week_01_Eda_02.html#scatterplot",
    "href": "lectures/Week_01/Week_01_Eda_02.html#scatterplot",
    "title": "Lab 02",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nThe most conventional way to display relationships between two variables is a scatterplot.\nIt shows the direction and strength of association between two variables.\n\n\n#https://seaborn.pydata.org/generated/seaborn.scatterplot.html\n#get a scatter plot of out of state tution fee and graduation rate\nsns.scatterplot(data = college_df, x = \"Outstate\", y = \"Grad.Rate\");\n#As tuition fee increases, the high graduation rate increases\n\n\n\n\n\n#how graph above chages with respect to school type?\nsns.scatterplot(data = college_df, x = 'Outstate', y = \"Grad.Rate\", hue = 'Private');"
  },
  {
    "objectID": "lectures/Week_01/Week_01_Eda_02.html#tools-for-displaying-more-than-two-variables",
    "href": "lectures/Week_01/Week_01_Eda_02.html#tools-for-displaying-more-than-two-variables",
    "title": "Lab 02",
    "section": "Tools for Displaying More Than Two Variables",
    "text": "Tools for Displaying More Than Two Variables\n\nScatterplot Matrix\n\nDisplaying more than two variables on a single scatterplot is not possible.\nA scatterplot matrix is one possible visualization of three or more continuous variables taken two at a time.\n\n\n#https://seaborn.pydata.org/generated/seaborn.pairplot.html\n#let's get a pairwise matrix plot of some variables. \n#diag_kind = None, otherwise histogram appears on the diagonals.\nsns.pairplot(college_df, vars = ['Outstate', 'Grad.Rate', 'Personal'], diag_kind = None, hue = 'Private');\n\n\n\n\n\nimport numpy as np\ndf = college_df[['Outstate', 'Grad.Rate', 'Personal']]\ndf.corr()\n\n\n\n\n\n  \n    \n      \n      Outstate\n      Grad.Rate\n      Personal\n    \n  \n  \n    \n      Outstate\n      1.000000\n      0.571290\n      -0.299087\n    \n    \n      Grad.Rate\n      0.571290\n      1.000000\n      -0.269344\n    \n    \n      Personal\n      -0.299087\n      -0.269344\n      1.000000\n    \n  \n\n\n\n\n\nimport session_info\nsession_info.show()\n\n\nClick to view session information\n-----\nmatplotlib          3.4.3\nnumpy               1.21.2\npandas              1.3.3\nseaborn             0.11.2\nsession_info        1.0.0\n-----\n\n\nClick to view modules imported as dependencies\nPIL                         8.3.2\nanyio                       NA\nappnope                     0.1.2\nattr                        21.2.0\nbabel                       2.9.1\nbackcall                    0.2.0\nbeta_ufunc                  NA\nbinom_ufunc                 NA\nbrotli                      1.0.9\ncertifi                     2021.05.30\ncffi                        1.14.6\nchardet                     4.0.0\ncharset_normalizer          2.0.0\ncolorama                    0.4.4\ncycler                      0.10.0\ncython_runtime              NA\ndateutil                    2.8.2\ndebugpy                     1.4.1\ndecorator                   5.1.0\ndefusedxml                  0.7.1\nentrypoints                 0.3\ngoogle                      NA\nidna                        3.1\nipykernel                   6.4.1\nipython_genutils            0.2.0\nipywidgets                  7.6.5\njedi                        0.18.0\njinja2                      3.0.1\njson5                       NA\njsonschema                  3.2.0\njupyter_server              1.11.0\njupyterlab_server           2.8.1\nkiwisolver                  1.3.2\nmarkupsafe                  2.0.1\nmatplotlib_inline           NA\nmpl_toolkits                NA\nnbclassic                   NA\nnbformat                    5.1.3\nnbinom_ufunc                NA\npackaging                   21.3\nparso                       0.8.2\npexpect                     4.8.0\npickleshare                 0.7.5\npkg_resources               NA\nprometheus_client           NA\nprompt_toolkit              3.0.20\nptyprocess                  0.7.0\npvectorc                    NA\npydev_ipython               NA\npydevconsole                NA\npydevd                      2.4.1\npydevd_concurrency_analyser NA\npydevd_file_utils           NA\npydevd_plugins              NA\npydevd_tracing              NA\npygments                    2.10.0\npyparsing                   2.4.7\npyrsistent                  NA\npytz                        2021.1\nrequests                    2.26.0\nscipy                       1.7.1\nsend2trash                  NA\nsix                         1.16.0\nsniffio                     1.2.0\nsocks                       1.7.1\nstatsmodels                 0.13.2\nstoremagic                  NA\nterminado                   0.12.1\ntornado                     6.1\ntraitlets                   5.1.0\nurllib3                     1.26.7\nwcwidth                     0.2.5\nwebsocket                   0.57.0\nzmq                         22.3.0\n\n \n-----\nIPython             7.27.0\njupyter_client      7.0.3\njupyter_core        4.8.1\njupyterlab          3.1.12\nnotebook            6.4.4\n-----\nPython 3.8.12 | packaged by conda-forge | (default, Sep 16 2021, 01:59:00) [Clang 11.1.0 ]\nmacOS-10.15.7-x86_64-i386-64bit\n-----\nSession information updated at 2022-09-20 00:03"
  }
]